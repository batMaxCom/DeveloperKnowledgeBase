# Оптимизация хранения в PostgreSQL

## Partitioning — партиционирование таблиц

Партиционирование = разделение одной большой таблицы на несколько частей (partitions), но логически это всё равно одна таблица.

Используют при таблицах размером больше `30–50 млн строк` или больше `50–100 GB`.

## Виды партиционирования в PostgreSQL
### RANGE (по диапазону)

Пример: партиционирование логов по месяцам.

CREATE TABLE logs (
    id serial,
    created_at date,
    message text
) PARTITION BY RANGE (created_at);


Создаем партиции:
```sql
CREATE TABLE logs_2025_01 PARTITION OF logs
FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE logs_2025_02 PARTITION OF logs
FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');
```

Запрос:
```sql
SELECT * FROM logs
WHERE created_at BETWEEN '2025-02-15' AND '2025-02-20';
```

Postgres обращается только к logs_2025_02 → Big Win.

### LIST (по списку значений)

Например — разносить пользователей по странам.
```sql
CREATE TABLE users (
    id serial,
    country text
) PARTITION BY LIST (country);
```
### HASH (редко, на равномерные нагрузки)

Когда партиционирование НЕ надо?
- таблица < 10 млн строк
- нет запросов с условиями по ключу партиционирования
- много JOIN — партиционированные таблицы иногда работают медленнее

## Clustering — физическая сортировка таблицы
CLUSTER перестраивает таблицу физически в порядке выбранного индекса.

Пример:
```sql
CLUSTER orders USING idx_orders_created_at;
```

Эффект:
- строки с близкими значениями ключа рядом на диске
- быстрее range-запросы по этому ключу
- улучшается чтение по ORDER BY created_at

Важно:
- это одноразовая операция — таблица снова "фрагментируется" со временем
- чтобы сохранять порядок, используют pg_repack, но это отдельная утилита

Когда использовать CLUSTER?

Использовать, если:
- таблица активно читается по одному ключу
- таблица редко обновляется
- таблица большая (1–200 GB)

Не использовать если:
- часто идут UPDATE/INSERT
- нет явного ключа сортировки

## Оптимизация больших таблиц (> 100 млн строк)
1. Уменьшать типы

Плохой вариант:
```sql
user_id BIGINT
price NUMERIC(20,8)
```

Хороший:
```sql
user_id INT
price NUMERIC(10,2)
```

Каждый лишний байт × миллионы строк = гигабайты.

2. Избегать больших JSONB-схем

Плохой вариант — хранить 100 полей в одном JSONB.

Хороший — вынести часть атрибутов в отдельную таблицу.

3. Архивация данных
Таблица logs в 3 млрд строк?

- перенести логи старше 180 дней в архивную таблицу
- либо в S3 / ClickHouse

4. Использование BRIN индексов
Если данные "естественно" отсортированы (created_at):
```sql
CREATE INDEX idx_logs_created_at ON logs
USING brin (created_at);
```

BRIN:
- размер: килобайты
- ускоряет seq scan в 3–20 раз
- идеален для таблиц 100GB+

5. Автоматическая очистка (autovacuum tuning)